{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/opt/ml/sy_code/K-Fashion 이미지/Training/라벨링데이터'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8b44021515e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mjson_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABELING_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_LABELING_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALID_LABELING_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_json_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_LABELING_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mvalid_json_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALID_LABELING_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/opt/ml/sy_code/K-Fashion 이미지/Training/라벨링데이터'"
     ]
    }
   ],
   "source": [
    "TRAIN_LABELING_DIR = r\"/opt/ml/sy_code/K-Fashion 이미지/Training/라벨링데이터\"\n",
    "VALID_LABELING_DIR = r\"/opt/ml/sy_code/K-Fashion 이미지/Validation/라벨링데이터\"\n",
    "TRAIN_IMAGE_DIR = r\"/opt/ml/sy_code/K-Fashion 이미지/Training/원천데이터\"\n",
    "VALID_IMAGE_DIR = r\"/opt/ml/sy_code/K-Fashion 이미지/Validation/원천데이터\"\n",
    "\n",
    "train_json_files = []\n",
    "valid_json_files = []\n",
    "\n",
    "for train_file, valid_file in zip(os.listdir(TRAIN_LABELING_DIR), os.listdir(VALID_LABELING_DIR)):\n",
    "    train_json_files.append(os.path.join(TRAIN_LABELING_DIR, train_file))\n",
    "    valid_json_files.append(os.path.join(VALID_LABELING_DIR, valid_file))\n",
    "\n",
    "print(\"훈련 데이터 개수:\", len(train_json_files))\n",
    "print(\"평가 데이터 개수:\", len(valid_json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@기획샘플@-ops4537k04(2)-바이보늬(샘플,주름멜빵)-얼굴컷,어두운거는 밝게 보정해주시고 치마가 예쁘게 나온걸로 사진 많이 넣어주세여~-IMG_E0624.JPG\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-912e89258a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mimage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfabric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_json_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "clothes_class = [\"아우터\", \"하의\", \"원피스\", \"상의\"]\n",
    "clothes_xy = {\"outer_xy\":None, \"pants_xy\":None, \"onepiece_xy\":None, \"shirt_xy\":None}\n",
    "clothes_fabric = {\"outer_fabric\":None, \"pants_fabric\":None, \"onepiece_fabric\":None, \"shirt_fabric\":None}\n",
    "\n",
    "train_image_dict = {}\n",
    "valid_image_dict = {}\n",
    "\n",
    "for file in train_json_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = json.load(f)\n",
    "        file_name = data['이미지 정보']['이미지 파일명']\n",
    "        for cls, xy, fabric in zip(clothes_class, clothes_xy, clothes_fabric):\n",
    "            clothes_xy[xy] = data['데이터셋 정보']['데이터셋 상세설명']['렉트좌표'][cls][0].values()\n",
    "            if not clothes_xy[xy]:\n",
    "                continue\n",
    "            try:\n",
    "                clothes_fabric[fabric] = data['데이터셋 정보']['데이터셋 상세설명']['라벨링'][cls][0]['소재']\n",
    "            except:\n",
    "                print(file_name)\n",
    "                print(\"--------------------\")\n",
    "\n",
    "    for xy, fabric in zip(clothes_xy.values(), clothes_fabric.values()):\n",
    "        if not xy:\n",
    "            continue\n",
    "        train_image_dict[(file_name, xy)] = fabric[0]\n",
    "\n",
    "for file in valid_json_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = json.load(f)\n",
    "        file_name = data['이미지 정보']['이미지 파일명']\n",
    "        for cls, xy, fabric in zip(clothes_class, clothes_xy, clothes_fabric):\n",
    "            clothes_xy[xy] = data['데이터셋 정보']['데이터셋 상세설명']['렉트좌표'][cls][0].values()\n",
    "            if not clothes_xy[xy]:\n",
    "                continue\n",
    "            try:\n",
    "                clothes_fabric[fabric] = data['데이터셋 정보']['데이터셋 상세설명']['라벨링'][cls][0]['소재']\n",
    "            except:\n",
    "                print(file_name)\n",
    "                print(\"--------------------\")\n",
    "\n",
    "    for xy, fabric in zip(clothes_xy.values(), clothes_fabric.values()):\n",
    "        if not xy:\n",
    "            continue\n",
    "        valid_image_dict[(file_name, xy)] = fabric[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothDataset(Dataset):\n",
    "    def __init__(self, image_dict, data_dir=r\"/opt/ml/sy_code/원천데이터\"):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            image_dict:Dict[(image_file_name, rect_xy):fabric]\n",
    "            data_dir:Image path\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.image_dict = image_dict\n",
    "        self.image_files = []\n",
    "        self.dict_label_to_num = {\n",
    "            \"퍼\" : 0,\n",
    "            \"니트\" : 1,\n",
    "            \"무스탕\" : 2,\n",
    "            \"레이스\" : 3,\n",
    "            \"스웨이드\" : 4,\n",
    "            \"린넨\" : 5,\n",
    "            \"앙고라\" : 6,\n",
    "            \"메시\" : 7,\n",
    "            \"코듀로이\" : 8,\n",
    "            \"플리스\" : 9,\n",
    "            \"시퀸/글리터\" : 10,\n",
    "            \"네오프렌\" : 11,\n",
    "            \"데님\" : 12,\n",
    "            \"실크\" : 13,\n",
    "            \"저지\" : 14,\n",
    "            \"스판덱스\" : 15,\n",
    "            \"트위드\" : 16,\n",
    "            \"자카드\" : 17,\n",
    "            \"벨벳\" : 18,\n",
    "            \"가죽\" : 19,\n",
    "            \"비닐/PVC\" : 20,\n",
    "            \"면\" : 21,\n",
    "            \"울/캐시미어\" : 22,\n",
    "            \"시폰\" : 23,\n",
    "            \"합성섬유\" : 24,\n",
    "            \"우븐\" : 25\n",
    "        }\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        for (file_name, xy), fabric in self.image_dict.items():\n",
    "            if not fabric:\n",
    "                continue\n",
    "            self.image_files.append((os.path.join(self.data_dir, file_name), xy, fabric))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name, xy, fabric = self.image_files[idx]\n",
    "\n",
    "        fabric = fabric[0]\n",
    "            \n",
    "        x, y, w, h = map(int, xy)\n",
    "        im = Image.open(file_name)\n",
    "        im = transforms.ToTensor()(im)\n",
    "        im = torchvision.transforms.functional.crop(im, y, x, h, w)\n",
    "        im = transforms.Resize((300, 300))(im)\n",
    "\n",
    "        fabric_label = self.dict_label_to_num[fabric]\n",
    "        return im, fabric_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothModel(nn.Module):\n",
    "    def __init__(self, num_classes=26):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(pretrained=True)\n",
    "        self.fc = nn.Linear(1000, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_image_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-6d2a4d5a325c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test_dataset = ClothDataset(image_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClothDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClothDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_image_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(dataset[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_image_dict' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = ClothDataset(train_image_dict)\n",
    "valid_dataset = ClothDataset(valid_image_dict)\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:00<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5](3/14) || training loss 3.482 || training accuracy 33.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [00:00<00:00, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5](6/14) || training loss 7.961 || training accuracy 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [00:00<00:00, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5](9/14) || training loss 13.94 || training accuracy 108.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 12.68it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5](12/14) || training loss 17.65 || training accuracy 141.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:00<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5](3/14) || training loss 1.956 || training accuracy 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [00:00<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5](6/14) || training loss 4.289 || training accuracy 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5](9/14) || training loss 6.585 || training accuracy 133.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 12.50it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5](12/14) || training loss 8.366 || training accuracy 183.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:00<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2/5](3/14) || training loss 1.824 || training accuracy 75.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [00:00<00:00, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2/5](6/14) || training loss 4.735 || training accuracy 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [00:00<00:00, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2/5](9/14) || training loss 5.908 || training accuracy 150.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 12.68it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2/5](12/14) || training loss 7.174 || training accuracy 183.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:00<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/5](3/14) || training loss 0.9692 || training accuracy 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [00:00<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/5](6/14) || training loss 3.878 || training accuracy 116.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [00:00<00:00, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/5](9/14) || training loss 5.585 || training accuracy 166.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 12.76it/s]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/5](12/14) || training loss 7.019 || training accuracy 225.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:00<00:01,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/5](3/14) || training loss 1.246 || training accuracy 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [00:00<00:00, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/5](6/14) || training loss 2.022 || training accuracy 150.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [00:00<00:00, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/5](9/14) || training loss 3.267 || training accuracy 225.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 11.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/5](12/14) || training loss 4.535 || training accuracy 275.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LOG_INTERVAL = 100\n",
    "LEARNING_RATE = 0.001\n",
    "SAVE_DIR = \"./results\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=False)\n",
    "\n",
    "model = ClothModel()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for idx, batch in enumerate(tqdm(train_loader)):\n",
    "        inputs, labels = batch\n",
    " \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_value += loss.item()  # loss값 저장\n",
    "        matches += (preds == labels).sum().item()  # accuracy값 저장\n",
    "        if (idx + 1) % LOG_INTERVAL == 0:\n",
    "            train_loss = loss_value / LOG_INTERVAL\n",
    "            train_acc = matches / BATCH_SIZE / LOG_INTERVAL\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{EPOCHS}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%}\"\n",
    "            )\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        # figure = None\n",
    "        \n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "            # if figure is None:\n",
    "            #     inputs_np = torch.clone(inputs).detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "            #     inputs_np = dataset_module.denormalize_image(inputs_np, dataset.mean, dataset.std)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(valid_dataset)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "            torch.save(model.module.state_dict(), f\"{SAVE_DIR}/best.pth\")\n",
    "            best_val_acc = val_acc\n",
    "        torch.save(model.module.state_dict(), f\"{SAVE_DIR}/last.pth\")\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-67-1dffb63e7b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(train_dataset[9][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['클래식',\n",
       " '매니시',\n",
       " '페미닌',\n",
       " '히피',\n",
       " '모던',\n",
       " '컨트리',\n",
       " '젠더리스',\n",
       " '스포티',\n",
       " '레트로',\n",
       " '밀리터리',\n",
       " '프레피',\n",
       " '톰보이',\n",
       " '로맨틱',\n",
       " '웨스턴',\n",
       " '소피스트',\n",
       " '케이티드',\n",
       " '리조트',\n",
       " '키치/',\n",
       " '키덜트',\n",
       " '스트리트',\n",
       " '섹시',\n",
       " '오리엔탈',\n",
       " '아방가르드',\n",
       " '힙합',\n",
       " '펑크']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"클래식\t매니시\t페미닌\t히피\t모던\t컨트리\t젠더리스\t스포티\t레트로\t밀리터리\n",
    "프레피\t톰보이\t로맨틱\t웨스턴\t소피스트\n",
    "케이티드\t리조트\t \t \t키치/\n",
    "키덜트\t스트리트\n",
    " \t \t섹시\t오리엔탈\t아방가르드\t \t \t \t힙합\t \n",
    " \t \t \t \t \t \t \t \t펑크\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
